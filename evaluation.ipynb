{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load two files:\n",
    "model_outputs = json.load(open('/Users/venkatakesavvenna/Sem6/RSAI/Mid_Project/dataset/nli_output/mistral_7b_checker_final.json'))\n",
    "human_outputs = json.load(open('/Users/venkatakesavvenna/Sem6/RSAI/Mid_Project/dataset/human_annotations/zero_context/nq_mistral_7b_answers.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'response', 'triplets', 'reference', 'Y', 'ys'])\n",
      "dict_keys(['id', 'response', 'claude2_response_kg'])\n"
     ]
    }
   ],
   "source": [
    "# Print the keys \n",
    "print(model_outputs[0].keys())\n",
    "print(human_outputs[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an array of the form [triplet, model_output, human_output]\n",
    "final_array = []\n",
    "\n",
    "# Check the ids, that are common in both the files\n",
    "for model_output in model_outputs:\n",
    "    for human_output in human_outputs:\n",
    "        if model_output['id'] == human_output['id']:\n",
    "            final_array.append([model_output[\"triplets\"], model_output[\"ys\"], [entry[\"human_label\"] for entry in human_output[\"claude2_response_kg\"]]])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[22.  3.  9.]\n",
      " [ 8. 59.  9.]\n",
      " [ 6.  4. 43.]]\n",
      "\n",
      "Accuracy: 0.7607361963190185\n",
      "\n",
      "Category: Neutral\n",
      "Precision: 0.6111111111111112\n",
      "Recall: 0.6470588235294118\n",
      "F1-score: 0.6285714285714287\n",
      "\n",
      "Category: Entailment\n",
      "Precision: 0.8939393939393939\n",
      "Recall: 0.7763157894736842\n",
      "F1-score: 0.8309859154929577\n",
      "\n",
      "Category: Contradiction\n",
      "Precision: 0.7049180327868853\n",
      "Recall: 0.8113207547169812\n",
      "F1-score: 0.7543859649122806\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Categories\n",
    "categories = ['Neutral', 'Entailment', 'Contradiction']\n",
    "\n",
    "# Initialize confusion matrix\n",
    "conf_matrix = np.zeros((len(categories), len(categories)))\n",
    "\n",
    "# Count occurrences of each combination of model and human outputs\n",
    "for entry in final_array:\n",
    "    model_outputs = entry[1]\n",
    "    human_outputs = entry[2]\n",
    "\n",
    "    for model_output, human_output in zip(model_outputs, human_outputs):\n",
    "        # Get indices of categories\n",
    "        model_index = categories.index(model_output)\n",
    "        human_index = categories.index(human_output)\n",
    "\n",
    "        # Update confusion matrix\n",
    "        conf_matrix[model_index][human_index] += 1\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.trace(conf_matrix) / np.sum(conf_matrix)\n",
    "print(f\"\\nAccuracy: {accuracy}\")\n",
    "\n",
    "# Calculate precision, recall, and F1-score for each category\n",
    "for i, category in enumerate(categories):\n",
    "    true_positives = conf_matrix[i][i]\n",
    "    false_positives = sum(conf_matrix[:, i]) - true_positives\n",
    "    false_negatives = sum(conf_matrix[i, :]) - true_positives\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    print(f\"\\nCategory: {category}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1-score: {f1_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/venkatakesavvenna/Sem6/RSAI/Mid_Project/dataset/nli_output/alpaca-7b_checker_final.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Analysis for Alpaca-7B model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model_outputs \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39m/Users/venkatakesavvenna/Sem6/RSAI/Mid_Project/dataset/nli_output/alpaca-7b_checker_final.json\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m      4\u001b[0m \u001b[39m# Make an array of the form [triplet, model_output, human_output]\u001b[39;00m\n\u001b[1;32m      5\u001b[0m final_array \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/venkatakesavvenna/Sem6/RSAI/Mid_Project/dataset/nli_output/alpaca-7b_checker_final.json'"
     ]
    }
   ],
   "source": [
    "# Analysis for Alpaca-7B model\n",
    "model_outputs = json.load(open('dataset/human_annotations/zero_context/nq_alpaca_7B_answers.json'))\n",
    "human_outputs = json.load(open('dataset/nli_output/alpaca_7b_checker_final.json'))\n",
    "\n",
    "# Make an array of the form [triplet, model_output, human_output]\n",
    "final_array = []\n",
    "\n",
    "# Take the majority votes for the human outputs\n",
    "for model_output in model_outputs:\n",
    "    for human_output in human_outputs:\n",
    "        if model_output['id'] == human_output['id']:\n",
    "            model_final = [max(set(model_output[\"ys\"]), key = model_output[\"ys\"].count)]\n",
    "            human_final = [max(set([entry[\"human_label\"] for entry in human_output[\"claude2_response_kg\"]]), key = [entry[\"human_label\"] for entry in human_output[\"claude2_response_kg\"]].count)]\n",
    "\n",
    "            final_array.append([model_output[\"triplets\"], model_final, human_final])\n",
    "\n",
    "# Check if the models, human outputs are matching\n",
    "matching = 0\n",
    "\n",
    "for entry in final_array:\n",
    "    if entry[1] == entry[2]:\n",
    "        matching += 1\n",
    "\n",
    "print(f\"Matching: {matching} out of {len(final_array)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
