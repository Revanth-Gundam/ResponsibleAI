{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load two files:\n",
    "model_outputs = json.load(open('/Users/venkatakesavvenna/Sem6/RSAI/Mid_Project/dataset/nli_output/llama-70b_checker_final.json'))\n",
    "human_outputs = json.load(open('/Users/venkatakesavvenna/Sem6/RSAI/Mid_Project/dataset/human_annotations/zero_context/nq_llama2_70b_chat_answers.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'response', 'triplets', 'reference', 'Y', 'ys'])\n",
      "dict_keys(['id', 'response', 'claude2_response_kg'])\n"
     ]
    }
   ],
   "source": [
    "# Print the keys \n",
    "print(model_outputs[0].keys())\n",
    "print(human_outputs[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an array of the form [triplet, model_output, human_output]\n",
    "final_array = []\n",
    "\n",
    "# Check the ids, that are common in both the files\n",
    "for model_output in model_outputs:\n",
    "    for human_output in human_outputs:\n",
    "        if model_output['id'] == human_output['id']:\n",
    "            final_array.append([model_output[\"triplets\"], model_output[\"ys\"], [entry[\"human_label\"] for entry in human_output[\"claude2_response_kg\"]]])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[565.  16.  39.]\n",
      " [155. 210.  25.]\n",
      " [ 50.  17.  66.]]\n",
      "\n",
      "Category: Neutral\n",
      "Precision: 0.7337662337662337\n",
      "Recall: 0.9112903225806451\n",
      "F1-score: 0.8129496402877697\n",
      "\n",
      "Category: Entailment\n",
      "Precision: 0.8641975308641975\n",
      "Recall: 0.5384615384615384\n",
      "F1-score: 0.6635071090047393\n",
      "\n",
      "Category: Contradiction\n",
      "Precision: 0.5076923076923077\n",
      "Recall: 0.49624060150375937\n",
      "F1-score: 0.5019011406844105\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Categories\n",
    "categories = ['Neutral', 'Entailment', 'Contradiction']\n",
    "\n",
    "# Initialize confusion matrix\n",
    "conf_matrix = np.zeros((len(categories), len(categories)))\n",
    "\n",
    "# Count occurrences of each combination of model and human outputs\n",
    "for entry in final_array:\n",
    "    model_outputs = entry[1]\n",
    "    human_outputs = entry[2]\n",
    "\n",
    "    for model_output, human_output in zip(model_outputs, human_outputs):\n",
    "        # Get indices of categories\n",
    "        model_index = categories.index(model_output)\n",
    "        human_index = categories.index(human_output)\n",
    "\n",
    "        # Update confusion matrix\n",
    "        conf_matrix[model_index][human_index] += 1\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate precision, recall, and F1-score for each category\n",
    "for i, category in enumerate(categories):\n",
    "    true_positives = conf_matrix[i][i]\n",
    "    false_positives = sum(conf_matrix[:, i]) - true_positives\n",
    "    false_negatives = sum(conf_matrix[i, :]) - true_positives\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    print(f\"\\nCategory: {category}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1-score: {f1_score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
